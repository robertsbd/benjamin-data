---
title: "Visualising SQL Lineage in Spark with Neo4j for Data Warehouse Build-Outs"
date: 2025-05-06
---

INSERT TABLE LEVEL LINEAGE

In the middle of a complex data warehouse build for a client using Microsoft Fabric, we needed clear visibility into how data was moving and transforming across layers—especially as SQL code was evolving rapidly across sprints. Our codebase lives in GitHub, and we use Spark to execute SQL workflows—not PySpark DataFrames, but actual SQL transformations executed in Spark notebooks. To ensure quality and understand impact, we needed column level lineage—and we needed it automated.

Column level lineage doesn't exist in Microsoft Fabric so it was necessary to build it out ourselves, and in addition for users of Databricks and Snowflake it is worth considering using a Graph Database for maintaining your lineage mappings given the flexibility it offers over using the inbuilt lineage functions.

We needed lineage so that we could
- Track what we were doing 
- Provide delivery information to the project to indicate what had been built and what was being built
- Build out documentation for ourselves and for the client's data engineering teams

Here's how we tackled it.

At the end of each sprint, using Github APIs in Pyspark notebooks in Fabric, we extract all SQL code merged into the develop branch of our GitHub repository. These SQL scripts live inside .Notebook folders and follow a standard format, using custom --BEGIN_LINEAGE and --END_LINEAGE tags to demarcate analyzable SQL blocks.

Once we fetch these blocks, we use sqlglot to parse and standardise the SQL ASTs and then inspect each CREATE TABLE AS SELECT or CREATE VIEW AS SELECT operation to map out which source columns contribute to each sink column. We're not looking at DataFrame APIs here—this is native SQL, parsed and processed on the Spark platform directly.

The output from this lineage analysis is saved into a Delta table for presentation in two ways, as Power BI reports, and as a Graph database. A full full graph model is constructed: databases → tables → columns, with CHILD relationships for hierarchy and CREATES edges to show lineage from source to sink, both table and column lineage. This is exported as Cypher (CQL) and imported into a Neo4j instance for interactive visualisation.

This graph is useful for impact analysis, onboarding, and debugging. Most importantly, it's integrated into our delivery rhythm—automatically updated at the end of each sprint, always reflecting the latest version of the SQL layer of the warehouse.

We're excited to keep refining it—adding error handling, incremental updates, and deeper lineage resolution. For now, it’s delivering huge value with minimal manual effort.

INSERT COLUMN LEVEL LINEAGE PIC and talk about how by searching for a node you can then return all other columns associated with the lineage of the target column and all the tables, databases which are parents associated with the lineage path.
